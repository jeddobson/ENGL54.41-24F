{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f30f094-faec-427d-9188-bcafd5139d10",
   "metadata": {},
   "source": [
    "# <center>Critical AI</center>\n",
    "<center>ENGL 54.41</center>\n",
    "<center>Dartmouth College</center>\n",
    "<center>Fall 2024</center>\n",
    "\n",
    "<center><h2>Midterm Exam</h2></center>\n",
    "<hr>\n",
    "<pre>Created: 10/11/2024</pre>\n",
    "\n",
    "\n",
    "Due: 11:59pm Wednesday, October 23rd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fd4c4e-ab59-4b79-b636-56db0b2d4925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from glob import glob\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy, os, re\n",
    "import torch, torchvision\n",
    "from torchvision.models import vgg16\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "import sklearn\n",
    "\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612ebb5a-8e59-44e1-96a0-67cfd9b1ef92",
   "metadata": {},
   "source": [
    "**Question #1**: Critical AI\n",
    "\n",
    "What are the meaningful differences and similarities between two different (select accounts from different essays/chapters) definitions of \"critical AI\" found in our readings thus far? (Approx. 500 words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43282326-1a60-4bf0-90f3-635ea3469553",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1c843b49-c79d-4f4c-9c0c-b59c3b8105e1",
   "metadata": {},
   "source": [
    "**Question #2**: On Neural networks and learning. \n",
    "\n",
    "Using Frank Rosenblatt's own account of the Perceptron in “Design of an Intelligent Automaton,” explain what he means by learning. Make sure to quote and cite his article. How does this approach compare with the everyday understanding of learning? (Approx. 250 words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc216c1e-3bee-4551-a202-d381a5fad2dd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87977fda-4818-46b8-8595-83a9711fa294",
   "metadata": {},
   "source": [
    "**Question #3**: Generalization in Convolutional Neural Networks. Explain how you understand generalization to work in convolutional neural networks and the differences and similarities of generalization to instrumental realism, as presented by Malevé and Sluis. (Approx 250 words)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b6a39115-6f46-46c5-8573-292735ff4091",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8e38590-8eb8-420b-bdb2-2a94bffd0d62",
   "metadata": {},
   "source": [
    "##### **Question #4**: Critique of Image Pipeline\n",
    "\n",
    "In Emily Denton, et al., “On the Genealogy of Machine Learning Datasets: A Critical History of ImageNet,” *Big Data & Society* 8, no. 2 (2021), the authors write: “We analyze discourses which shaped ImageNet, focusing on three problems: the importance of data; meaning and the computational construction of understanding; and the strategic choices regarding the visibility (and invisibility) of labor.\" Please first explain the significance and relation of these three elements and then apply this critical lens to your reading of the [\"Deep Learning Face Attributes in the Wild\"](https://liuziwei7.github.io/projects/FaceAttributes.html) paper and to the [CelebA dataset](https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html). (Approx 500 words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29c51ad-f8d4-4f1f-9fff-bc950d3a720c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e9ab2a5-006b-44a4-9057-57510e1a1d1b",
   "metadata": {},
   "source": [
    "**Question #5**: Applying Critique to VGG16.\n",
    "\n",
    "Upload an image, run the cells below, and explain in the markdown cell how and where all three problems mentioned above are found in the output of this pre-trained VGG16 model. (Approx. 250 words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f804ae5-d5af-41e8-8919-4dfd4587aee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vgg16(weights='DEFAULT')\n",
    "weights = torchvision.models.VGG16_Weights.DEFAULT\n",
    "preprocess = weights.transforms()\n",
    "\n",
    "def display_image(image_file):\n",
    "    img = np.asarray(Image.open(image_file))\n",
    "    plt.imshow(img)\n",
    "    \n",
    "def decode_predictions(prediction,topk):\n",
    "    # can't figure out how to make tensor's argsort reverse like this\n",
    "    # we should already be on the CPU but leaving as such for future use\n",
    "    class_ids = np.argsort(prediction.to('cpu').detach().numpy())[::-1][:topk]\n",
    "    for i in class_ids:\n",
    "        score = prediction[i].item()\n",
    "        category_name = weights.meta[\"categories\"][i]\n",
    "        print(f\"{100 * score:5.2f}% {category_name}\")\n",
    "\n",
    "def get_prediction(image_file,topk=5,display_flag=False):\n",
    "    img = read_image(image_file,mode=ImageReadMode.RGB)\n",
    "    batch = preprocess(img).unsqueeze(0)\n",
    "    # we should already be on the CPU but leaving as such for future use\n",
    "    prediction = model(batch.to('cpu')).squeeze(0).softmax(0)\n",
    "    if display_flag:\n",
    "        display_image(image_file)\n",
    "    decode_predictions(prediction,topk=topk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3b8c94-e11d-45fc-80ec-f4464a108ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_prediction(\"image.jpg\", display_flag=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5681cd-6270-4517-9e47-a0e889a9d80b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6d34ada-9c9e-45a4-97d9-229f5ca9c1cb",
   "metadata": {},
   "source": [
    "**Question #6**: Vector Space Similarities in Text Models.\n",
    "\n",
    "First modify the values of the list \"concept_a\" and \"concept_b\" to contain words that you believe to be associate with two different concepts. These should be single words and lowercase. Use fairly basic terms. You should have a minimum of five words in each list. What happens if you add words that seem like they could belong to either concept? Then, referencing Mitchell and Gavin, explain the meaning of the resulting scatterplot (Approx. 250 words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d8afb6-1a9d-4576-afb9-39901c4c8f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download model\n",
    "import gensim.downloader as api\n",
    "google_model = api.load(\"word2vec-google-news-300\")\n",
    "vocab_size, dim = google_model.vectors.shape\n",
    "\n",
    "print(f\"vocab size: {vocab_size}, vector width: {dim}\")\n",
    "\n",
    "concept_a = [\"word1\",\"word2\",\"word3\",\"word4\",\"word5\"]\n",
    "concept_b = [\"word1\",\"word2\",\"word3\",\"word4\",\"word5\"]\n",
    "\n",
    "vecs, terms = list(), list()\n",
    "for w in concept_a + concept_b:\n",
    "    if w in google_model:\n",
    "        vecs.append(google_model[w])\n",
    "        terms.append(w)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "plot_data = pca.fit_transform(vecs)\n",
    "xs, ys = plot_data[:, 0], plot_data[:, 1]\n",
    "\n",
    "fig = plt.figure(figsize=(20, 15))\n",
    "plt.title(\"Embedding Space: Concept A + B\")\n",
    "plt.scatter(xs, ys, marker = '^')\n",
    "for i, w in enumerate(terms):\n",
    "    plt.annotate(w, xy = (xs[i], ys[i]), xytext = (3, 3),\n",
    "                 textcoords = 'offset points', ha = 'left', va = 'top')\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8403cc6a-4390-4ec5-b468-c3e2ae48499a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
