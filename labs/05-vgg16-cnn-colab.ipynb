{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e6fc040-01f8-44ee-9c65-e24c0178f26a",
   "metadata": {},
   "source": [
    "# <center>Critical AI</center>\n",
    "<center>ENGL 54.41</center>\n",
    "<center>Dartmouth College</center>\n",
    "<center>Fall 2024</center>\n",
    "<pre>Created: 04/29/2023; Revised: 10/02/2024</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c7b8fe-eafe-4131-beed-67b6fb80720e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook uses the VGG16 neural network (https://www.robots.ox.ac.uk/~vgg/) \n",
    "# VGG16 was trained in ImageNet and can provide object detection, which can\n",
    "# be useful with these images and the network's representation of scaled images\n",
    "# as a vector of 1,000 values provides useful features for distance measurements,\n",
    "# classifications, and other image tasks. \n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from glob import glob\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy, os, re\n",
    "import torch, torchvision\n",
    "from torchvision.models import vgg16\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0a643a-4196-4be4-b75d-6dd1cee4e150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model and load saved weights. We are loading a pre-trained model\n",
    "# that has been trained on ImageNet data. In our earlier lab, we worked in \n",
    "# two stages: training and testing. Note that we are not iterating through\n",
    "# training data but simply supplying data to be classified, as we did with \n",
    "# the testing stage of the previous neural networks.\n",
    "\n",
    "model = vgg16(weights='DEFAULT')\n",
    "weights = torchvision.models.VGG16_Weights.DEFAULT\n",
    "preprocess = weights.transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef36f1f9-2f9a-4af8-a349-e2590011285d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put model into eval state. This will display the model's architecture.\n",
    "# The architecture is the organization of the network. From the first object, we know\n",
    "# that this is a Sequential container:\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html\n",
    "# It is followed by a series of layers that perform a convolution on the input data.\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d\n",
    "# Each Convolution (Conv2d) is followed by an activation function (ReLU, or Rectified \n",
    "# Linear Unit, which we used previously in our Perceptron-style network).\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1847bac0-9ed0-4d9a-8710-3aebd813b3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights.meta.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8da446c-b88e-430a-b2c2-3fad5d89d06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This CNN network architecture will generate 1000 outputs, one for\n",
    "# each label in the training data. Like our earlier neural networks,\n",
    "# we will have output values that can be read as probabilities or\n",
    "# confidence scores in the classification.\n",
    "print(f'Total classes: {len(weights.meta[\"categories\"])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b7802e-878b-42b5-b778-e29be117fec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Ten sample classes: {random.sample(weights.meta[\"categories\"],k=10)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cd5d2a-2780-4037-917d-2dd106e2fabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "### Here we are defining a number of helper functions. We'll use these later ###\n",
    "#################################################################################\n",
    "\n",
    "def display_image(image_file):\n",
    "    img = np.asarray(Image.open(image_file))\n",
    "    plt.imshow(img)\n",
    "    \n",
    "def decode_predictions(prediction,topk):\n",
    "    # can't figure out how to make tensor's argsort reverse like this\n",
    "    # we should already be on the CPU but leaving as such for future use\n",
    "    class_ids = np.argsort(prediction.to('cpu').detach().numpy())[::-1][:topk]\n",
    "    for i in class_ids:\n",
    "        score = prediction[i].item()\n",
    "        category_name = weights.meta[\"categories\"][i]\n",
    "        print(f\"{100 * score:5.2f}% {category_name}\")\n",
    "\n",
    "def get_prediction(image_file,topk=5,display_flag=False):\n",
    "    img = read_image(image_file,mode=ImageReadMode.RGB)\n",
    "    batch = preprocess(img).unsqueeze(0)\n",
    "    # we should already be on the CPU but leaving as such for future use\n",
    "    prediction = model(batch.to('cpu')).squeeze(0).softmax(0)\n",
    "    if display_flag:\n",
    "        display_image(image_file)\n",
    "    decode_predictions(prediction,topk=topk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d15073-ab06-4a97-aecc-bee5dfbb0127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will load in a sample image.\n",
    "# This comes from the Vilhjalmur Stefansson collection in Rauner\n",
    "# https://www.library.dartmouth.edu/digital/digital-collections/vilhjalmur-stefansson-collection-arctic-photographs\n",
    "\n",
    "# download image\n",
    "!wget 'https://raw.githubusercontent.com/jeddobson/ENGL54.41-24F/6ebd8c683c3b0d230f16e99fdf7baa2113d10822/img/stefansson-arctic-image-rauner.png'\n",
    "\n",
    "# load image\n",
    "img = read_image('stefansson-arctic-image-rauner.png',mode=ImageReadMode.RGB)\n",
    "print(f'Found Image of the following dimensions: {img.shape}')\n",
    "\n",
    "# Why three dimensions? We used only grayscale images before with\n",
    "# pixel intensity values of 0-255. Now we have RGB color images with\n",
    "# three channels (R,G,B) of pixel intensity values that enable us\n",
    "# represent color images.\n",
    "# \n",
    "# But in order to display the image, we need to swap the dimensions:\n",
    "# x,y,z \n",
    "\n",
    "plt.imshow(img.permute(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81268d70-a013-4db8-8584-95a98795f14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we are going to preprocess this image to standardize the data\n",
    "# for our CNN network.\n",
    "img2 = preprocess(img).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e0ea19-de45-49cd-98bf-90c6d3924061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are our image dimensions now? Note the change from before. \n",
    "# What does this mean?\n",
    "img2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d8f33b-411b-4943-9b3c-2e3b81e32436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess() returns a 4d tensor. This enables us to pass multiple images\n",
    "# but we only want 3d data from the first image right now.\n",
    "plt.imshow(img2[0].permute(1,2,0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6247587e-a0e6-499e-9cbe-1af9cbdd0e1f",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Inputs for almost all machine learning and artificial intelligence systems need to be normalized. Think back to our MNIST dataset of handwritten digits or the CelebA dataset. The faces were all centered and the eyes aligned. The images were then cropped and resized to all be the identical size. The digits were all 28x28 pixel matrices. This normalization of data enables the same transformations to be applied to all images. Our above image has been resized to be 224x224 pixels--but has it been resized? What differences do you notice between the above image the initial image?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4beb20a-604a-4013-bdc1-25240ba3a4ac",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "We run *inference* on a model to obtain outputs from our inputs. In this case we are also going to run softmax on these outputs while assigning to our outputs variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4ededd-1ad8-406d-b166-4cb013264eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(img2.to('cpu')).squeeze(0).softmax(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24a11c0-6624-4e59-8b51-f7c2dc06f92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What shape are our outputs from the CNN? What does this \n",
    "# tell us?\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0845d1-f557-4930-9b52-ad2a772ff6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can map these values to our labels and sort them with \n",
    "# the following function:\n",
    "decode_predictions(outputs,topk=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3eb9e33-57bc-4a3a-9336-dc3e8d6e41d1",
   "metadata": {},
   "source": [
    "Do these classes and values make sense? Do they make less sense after a certain point? Do you have a sense of what visual features may have informed the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50cbcf1-4e5d-42a2-a851-0f44c9403cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see all the classes that could be used we can examine the values\n",
    "# held in the weights.meta dictionary as key 'categories'. Read through \n",
    "# these. Do they make sense to you? What do you notice?\n",
    "weights.meta[\"categories\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7624107b-72b4-466f-ad47-1f16ccac7e5e",
   "metadata": {},
   "source": [
    "## Putting it together!\n",
    "\n",
    "Okay, now we want to find an image, from your computer or off the internet. Go to a website that you like or use Google Image Search. Whatever. Download the image if needed, rename to something short (like testimage.png) if the name is wonky, and then upload to JupyterLab.\n",
    "\n",
    "Replace FILENAME below with the name of your file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c771c3-2bbc-4108-9a8c-a0248742bf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_prediction('FILENAME',topk=10,display_flag=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
