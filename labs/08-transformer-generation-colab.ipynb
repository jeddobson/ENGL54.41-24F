{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDUByRGu6Cr2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1Yf5VMD6VZD"
      },
      "outputs": [],
      "source": [
        "# load GPT2 model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\",\n",
        "                                             output_hidden_states=True,\n",
        "                                             device_map=\"auto\")\n",
        "\n",
        "# end of sentence/text token padding\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Uncommenting this will display the model's configuration\n",
        "model.config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xl8E99RP7Fyf"
      },
      "outputs": [],
      "source": [
        "def next_word_prediction_probs(prompt, n=10):\n",
        "    inp_tok = tokenizer(prompt,\n",
        "                        padding=True,\n",
        "                        return_tensors=\"pt\").to(next(model.parameters()).device)\n",
        "    input_ids = inp_tok[\"input_ids\"]\n",
        "    logits = model(**inp_tok).logits[:, -1, :]\n",
        "    vals = [[tokenizer.decode(tk.item()),\n",
        "             logits[0][tk.item()].tolist()] for tk in torch.argsort(logits, descending=True)[:, :n][0]]\n",
        "    return vals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HlQ0232pR1Vy"
      },
      "outputs": [],
      "source": [
        "next_word_prediction_probs(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUTlQsCn6rkL"
      },
      "outputs": [],
      "source": [
        "def next_tokens(prompt, n=10):\n",
        "    inp_tok = tokenizer(prompt,\n",
        "                        padding=True,\n",
        "                        return_tensors=\"pt\").to(next(model.parameters()).device)\n",
        "    input_ids = inp_tok[\"input_ids\"]\n",
        "    for i in range(n):\n",
        "        logits = model(input_ids).logits[:, -1, :]\n",
        "        pid = torch.argsort(logits, descending=True)[:, :1]\n",
        "        input_ids =  torch.cat((input_ids, pid),dim=1)\n",
        "    return tokenizer.decode(input_ids[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgufEYv26w6H"
      },
      "outputs": [],
      "source": [
        "next_tokens(\"\",n=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmbV7iCR68QK"
      },
      "outputs": [],
      "source": [
        "def next_tokens_mn_generation(prompt, n=10):\n",
        "    inp_tok = tokenizer(prompt,\n",
        "                        padding=True,\n",
        "                        return_tensors=\"pt\").to(next(model.parameters()).device)\n",
        "    input_ids = inp_tok[\"input_ids\"]\n",
        "\n",
        "    for i in range(n):\n",
        "        logits = model(input_ids).logits[:, -1, :]\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        pid = torch.multinomial(probs, num_samples=1)\n",
        "        input_ids =  torch.cat((input_ids, pid),dim=1)\n",
        "    return tokenizer.decode(input_ids[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LT3fZ8IE6-CA"
      },
      "outputs": [],
      "source": [
        "next_tokens_mn_generation(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "miyaFElXPcT_"
      },
      "outputs": [],
      "source": [
        "def next_tokens_generation(prompt, n=10,temperature=0.9):\n",
        "    inp_tok = tokenizer(prompt,\n",
        "                        padding=True,\n",
        "                        return_tensors=\"pt\").to(next(model.parameters()).device)\n",
        "    input_ids = inp_tok.input_ids\n",
        "    attention_mask = inp_tok[\"attention_mask\"]\n",
        "    output = model.generate(input_ids,\n",
        "                            do_sample=True,\n",
        "                            temperature=temperature,\n",
        "                            attention_mask=attention_mask,\n",
        "                            pad_token_id=tokenizer.eos_token_id,\n",
        "                            max_length=n)\n",
        "    return tokenizer.decode(output[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vag3nIJPmQI"
      },
      "outputs": [],
      "source": [
        "next_tokens_generation(\"\",n=128)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extract the embeddings for a sentence\n",
        "def get_sentence_embeddings(sentence):\n",
        "    inp_tok = tokenizer(sentence,\n",
        "             padding=True,\n",
        "             return_tensors=\"pt\").to(next(model.parameters()).device)\n",
        "    input_ids = inp_tok[\"input_ids\"]\n",
        "    output = model(input_ids)\n",
        "\n",
        "    # return tokenized text for indexing\n",
        "    tokenized_text = [tokenizer.decode(id).strip() for id in input_ids[0]]\n",
        "\n",
        "    # extract hidden states\n",
        "    embs = torch.stack(output['hidden_states'], dim=0)\n",
        "    embs = torch.squeeze(embs, dim=1)\n",
        "    embs = embs.permute(1,0,2)\n",
        "\n",
        "    # mean embeddings in the last four layers\n",
        "    vectors = [torch.mean(t[-4:], dim=0).to('cpu').detach().numpy() for t in embs]\n",
        "\n",
        "    return tokenized_text, input_ids, vectors\n"
      ],
      "metadata": {
        "id": "BsEPGrtgsEon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\"To carry a Line to haul some of the seal aboard\",\n",
        "            \"As a proof, he subjoined Friedemann's letter and seal\",\n",
        "            \"a seal, black cherry tree, balm of gilead tree\",\n",
        "            \"His Father bad him go and fetch home two Kine to seal\",\n",
        "            \"The house in which the salt works is carried on..is also called a seal\",\n",
        "            \"In estimating the capacity of a tank and its corresponding holder, due allowance must be made for the height of the dip or seal\"\n",
        "]"
      ],
      "metadata": {
        "id": "eY82IT6r_P_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer(\" seal\")"
      ],
      "metadata": {
        "id": "Ay46xjzHCH_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode([13810])"
      ],
      "metadata": {
        "id": "wqwCpGoSCFMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tidx = 13810\n",
        "dists = list()\n",
        "for sidx, sentence in enumerate(sentences):\n",
        "  t,i,v = get_sentence_embeddings(sentence)\n",
        "  dists.append(v[((i==tidx).nonzero().squeeze())[1].to('cpu').numpy()])"
      ],
      "metadata": {
        "id": "3eQX7mmmu27V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "pca = PCA(n_components = 2)\n",
        "plot_data = pca.fit_transform(dists)\n",
        "\n",
        "# extract x&y values\n",
        "xs, ys = plot_data[:, 0], plot_data[:, 1]\n",
        "\n",
        "# create labels\n",
        "labels = [\"seal\"] * plot_data.shape[0]\n",
        "\n",
        "# plot data\n",
        "fig = plt.figure(figsize=(20, 15))\n",
        "plt.clf()\n",
        "plt.title(\"Contextual Embeddings (PCA)\")\n",
        "plt.style.use('ggplot')\n",
        "plt.scatter(xs, ys, marker = '^')\n",
        "for i, w in enumerate(labels):\n",
        "     plt.annotate(w, xy = (xs[i], ys[i]), xytext = (3, 3),fontsize=14,\n",
        "                  textcoords = 'offset points',\n",
        "                  ha = 'left',\n",
        "                  va = 'top')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "koBqTyxYEsGb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}