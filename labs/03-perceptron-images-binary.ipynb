{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Critical AI</center>\n",
    "<center>ENGL 54.41</center>\n",
    "<center>Dartmouth College</center>\n",
    "<center>Fall 2024</center>\n",
    "\n",
    "<hr>\n",
    "\n",
    "Notes: This notebook creates a more advanced Perceptron-style neural network using Pytorch. We'll now generate loss information that will give us insight into the learning process. We will also use an optimizer on this loss data to adjust the weights of the network. This neural network has two outputs for binary classification of two figures from the MNIST dataset of handwritten digits. \n",
    "\n",
    "<pre>Created: 07/15/2024; Revised: 09/25/2024</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import struct\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell of code will determine if we have an accelerator for running\n",
    "# our neural networks.\n",
    "# mps == Apple Silicon device (MX series of Macbooks)\n",
    "# cuda == Compute Unified Device Architecture is a toolkit from Nvidia and means we have a GPU\n",
    "# cpu == Just using the general-purpose CPU for our calculations\n",
    "\n",
    "if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print('Using device: {0}'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the MNIST dataset from\n",
    "# http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "with open('train-images-idx3-ubyte','rb') as f:\n",
    "    magic, size = struct.unpack(\">II\", f.read(8))\n",
    "    nrows, ncols = struct.unpack(\">II\", f.read(8))\n",
    "    data = np.fromfile(f, dtype=np.dtype(np.uint8).newbyteorder('>'))\n",
    "    data = data.reshape((size, nrows, ncols))\n",
    "\n",
    "with open('train-labels-idx1-ubyte','rb') as f:\n",
    "    magic, size = struct.unpack(\">II\", f.read(8))\n",
    "    labels = np.fromfile(f, dtype=np.dtype(np.uint8).newbyteorder('>'))\n",
    "\n",
    "# display information about the dataset\n",
    "print(f'training samples: {data.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the testing data (stored separately from the training data)\n",
    "with open('t10k-images-idx3-ubyte','rb') as f:\n",
    "    magic, size = struct.unpack(\">II\", f.read(8))\n",
    "    nrows, ncols = struct.unpack(\">II\", f.read(8))\n",
    "    test_data = np.fromfile(f, dtype=np.dtype(np.uint8).newbyteorder('>'))\n",
    "    test_data = test_data.reshape((size, nrows, ncols))\n",
    "\n",
    "with open('t10k-labels-idx1-ubyte','rb') as f:\n",
    "    magic, size = struct.unpack(\">II\", f.read(8))\n",
    "    test_labels = np.fromfile(f, dtype=np.dtype(np.uint8).newbyteorder('>'))\n",
    "\n",
    "# display information about the dataset\n",
    "print(f'testing samples: {test_data.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display an image from training data from each class -- what do these look like?\n",
    "idxs = [labels.tolist().index(i) for i in range(10)]\n",
    "fig = plt.figure(figsize=(10, 5))  # width, height in inches\n",
    "for i,idx in enumerate(idxs):\n",
    "    img = fig.add_subplot(2, 5, i + 1)\n",
    "    img.imshow(data[idx].reshape(28,28).astype('uint8'), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# These are the images classes that we want to classify with the binary classifier\n",
    "################################################################################\n",
    "class_one = 0\n",
    "class_two = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training data and labels\n",
    "# reshape our selected training data, flattening the 28x28 matrix into a single vector\n",
    "y = [0] * data[labels == class_one].shape[0] + [1] * data[labels == class_two].shape[0]\n",
    "X = np.vstack((data[labels == class_one].reshape(data[labels == class_one].shape[0], 784),data[labels == class_two].reshape(data[labels == class_two].shape[0], 784)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display information about the dataset\n",
    "print(f'training samples: {X.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating testing data\n",
    "# reshape our selected testing data, flattening the 28x28 matrix into a single vector\n",
    "y_test = [0] * test_data[test_labels == class_one].shape[0] + [1] * test_data[test_labels == class_two].shape[0]\n",
    "X_test = np.vstack((test_data[test_labels == class_one].reshape(test_data[test_labels == class_one].shape[0], 784),test_data[test_labels == class_two].reshape(test_data[test_labels == class_two].shape[0], 784)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display information about the dataset\n",
    "print(f'testing samples: {X_test.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** Important Variables **\n",
    "\n",
    "# number of training iterations\n",
    "epochs = 10\n",
    "\n",
    "# learning rate \n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the Perceptron as three layers: \n",
    "# input, hidden, output\n",
    "class Perceptron(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Perceptron, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 128)    # our first layer (S) will take as input pixel data and output 128 values\n",
    "        self.layer2 = nn.Linear(128, 128)          # this is our hidden layer (A) that takes 128 input values and outputs the same\n",
    "        self.layer3 = nn.Linear(128, 2)            # this is our final layer (R) that will take 128 input values and output binary values\n",
    "\n",
    "    # define forward function with non-linear activiation. This is more complicated than the \n",
    "    # simple linear activation function that we used in the last notebook. The Rectified Linear Unit\n",
    "    # or ReLU function is applied to the weights (and later updated by the optimizer) as we push\n",
    "    # our data through the network.\n",
    "    def forward(self, inputs):\n",
    "        inputs = torch.relu(self.layer1(inputs))\n",
    "        inputs = torch.relu(self.layer2(inputs))\n",
    "        outputs = self.layer3(inputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model with input layer size dynamically set to length of data\n",
    "print(\"Creating neural network...\")\n",
    "input_size = X[0].shape[0]\n",
    "print(\"input layer size: {0}\".format(input_size))\n",
    "model = Perceptron(input_dim = input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move model to our special accelerator device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data and labels to Torch tensor datatype\n",
    "training_data = torch.FloatTensor(X)\n",
    "labels = torch.LongTensor(y)\n",
    "\n",
    "test_labels = torch.LongTensor(y_test)\n",
    "testing_data = torch.FloatTensor(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates loss entropy for classification tasks\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# the Adam optimizer adjusts weights using gradient optimization  \n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "model.train()\n",
    "\n",
    "# iterate through each of the training epochs\n",
    "for e in range(epochs):\n",
    "    model.zero_grad()\n",
    "    outputs = model(training_data.to(device))\n",
    "\n",
    "    # supply labels to CrossEntropyLoss\n",
    "    loss_train = loss_fn(outputs.to('cpu'), labels)\n",
    "    loss_train.backward()\n",
    "\n",
    "    # adjust weights\n",
    "    optimizer.step()\n",
    "\n",
    "    if e % 10 == 0:\n",
    "        print(\"Epoch: {0} Loss: {1:.4f}\".format(e,loss_train.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This places the model in evaluation state for testing\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to predict class from output\n",
    "def predict(input_data):\n",
    "    outputs = model(input_data.to(device))\n",
    "    pred = torch.argmax(outputs)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict entire testing dataset\n",
    "scores = list()\n",
    "for i, j in enumerate(test_labels):\n",
    "    pc = predict(testing_data[i])\n",
    "    scores.append([pc,test_labels[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate confusion matrix\n",
    "c0tp = len([x for x in scores if x[1] == 0 and x[0] == 0])\n",
    "c0fn = len([x for x in scores if x[1] == 0 and x[0] == 1])\n",
    "c1tn = len([x for x in scores if x[1] == 1 and x[0] == 1])\n",
    "c1fp = len([x for x in scores if x[1] == 1 and x[0] == 0])\n",
    "\n",
    "# calculate accuracy\n",
    "accuracy = np.round(len([x for x in scores if x[0] == x[1]]) / len(scores),3)\n",
    "\n",
    "# display confusion matrix\n",
    "print(f'total model accuracy {accuracy}')\n",
    "print('Confusion Matrix:')\n",
    "print(\"{0:5d} {1:10d} {2:5d}\".format(0,c0tp,c0fn))\n",
    "print(\"{0:5d} {1:10d} {2:5d}\".format(1,c1fp,c1tn))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
