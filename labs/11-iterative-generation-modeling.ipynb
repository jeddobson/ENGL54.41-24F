{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TZsd5egFYbdi"
   },
   "outputs": [],
   "source": [
    "!pip install -U langchain-dartmouth > /dev/null\n",
    "\n",
    "from langchain_dartmouth.llms import ChatDartmouth, DartmouthLLM\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2y2GS8I7ZcAr"
   },
   "outputs": [],
   "source": [
    "os.environ[\"DARTMOUTH_API_KEY\"] = \"KEY_GOES_HERE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O5GIgpToZvZS"
   },
   "outputs": [],
   "source": [
    "def format_prompt(prompt):\n",
    "    \"\"\"\n",
    "    This function will format a prompt into what is needed for LangChain to produce ChatML.\n",
    "    Args:\n",
    "       prompt: the text to be embedded as human prompt.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        SystemMessage(content=system_prompt),\n",
    "        HumanMessage(content=prompt),\n",
    "    ]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v5mmBGhhZiCV"
   },
   "outputs": [],
   "source": [
    "max_new_tokens = 1024\n",
    "top_p = 0.75 # If set to < 1, only the smallest set of most probable tokens with probabilities that add up to top_p or higher are kept for generation.\n",
    "temperature = 1.0 #  Strictly positive float value used to modulate the logits distribution. A value smaller than 1 decreases randomness (and vice versa), with 0 being equivalent to shifting all probability mass to the most likely token\n",
    "repetition_penalty = None\n",
    "\n",
    "system_prompt = \"Always assist with care, respect, and truth. Respond with utmost utility yet securely. Avoid harmful, unethical, prejudiced, or negative content. Ensure replies promote fairness and positivity.\"\n",
    "\n",
    "kwargs = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3QVVx0aHZkh_"
   },
   "outputs": [],
   "source": [
    "llm_chat = ChatDartmouth(model_name=\"llama-3-1-8b-instruct\",\n",
    "                    temperature = temperature,\n",
    "                    top_p = top_p,\n",
    "                    max_tokens = max_new_tokens,\n",
    "                    model_kwargs=kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uVbzYO7aaHjQ"
   },
   "outputs": [],
   "source": [
    "# define prompt for iteration\n",
    "prompt = \"Write a short story about a student's first year attending Dartmouth College. Use normal paragraph structure. Include experiences and events in each of the three major terms: fall, winter, and spring.\"\n",
    "\n",
    "# create list to store generated texts.\n",
    "outputs = list()\n",
    "\n",
    "# iterate through and save stories to output list\n",
    "for i in range(10):\n",
    "          outputs.append(llm_chat.invoke(format_prompt(prompt)).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DXEE4hTJhmV8"
   },
   "outputs": [],
   "source": [
    "# vectorize and create document-term matrix for modeling\n",
    "vectorizer = CountVectorizer(input='content',\n",
    "                             strip_accents='unicode',\n",
    "                             stop_words='english')\n",
    "dtm = vectorizer.fit_transform(outputs)\n",
    "idx2voc = {v:k for k, v in vectorizer.vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "alSEktmdhr4F",
    "outputId": "3115ece9-a1a0-4edf-99b7-850eb6cbbc85"
   },
   "outputs": [],
   "source": [
    "vocab_sums = dtm.sum(axis=0)\n",
    "sorted_vocab = [(v, vocab_sums[0, i]) for v, i in vectorizer.vocabulary_.items()]\n",
    "sorted_vocab = sorted(sorted_vocab, key = lambda x: x[1], reverse=True)\n",
    "\n",
    "# display top twenty-five words\n",
    "for i in range(25):\n",
    "    print(sorted_vocab[i][0],\"->\",sorted_vocab[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zWwvnGcCxhrM"
   },
   "outputs": [],
   "source": [
    "# small helper function to examining presence of specific words\n",
    "def term_debug(term):\n",
    "    if term in vectorizer.vocabulary_:\n",
    "        idx = vectorizer.vocabulary_[term]\n",
    "    else:\n",
    "        print(\"Error: {0} not on vocabulary\".format(term))\n",
    "        return\n",
    "    tc = int(np.sum(dtm,axis=0)[:, idx].item())\n",
    "    tm = float(np.mean(dtm,axis=0)[:, idx].item())\n",
    "    return pd.DataFrame({'Total Count':tc,'Mean Count':tm},  index=[term])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "id": "TYs7m0z9yC2N",
    "outputId": "ba3a02d1-2d78-4447-f04c-b9f0f58d6546"
   },
   "outputs": [],
   "source": [
    "term_debug('academic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a8cxg8Z5t04_",
    "outputId": "127b2b23-43aa-49a7-d179-5aa59093a66a"
   },
   "outputs": [],
   "source": [
    "for doc in range(dtm.shape[0]):\n",
    "  doc1 = dtm[doc].toarray()[0]\n",
    "  rest = dtm[np.arange(dtm.shape[0]) != doc, :].toarray()[0]\n",
    "\n",
    "  diff = np.abs(doc1 - rest)\n",
    "  top_diff_indices = np.argsort(diff)[-10:]\n",
    "\n",
    "  print(\"Document {0}: Top 10 different terms:\".format(doc))\n",
    "  for index in top_diff_indices:\n",
    "      print(idx2voc[index],diff[index])\n",
    "  print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "utH2GsiDick8",
    "outputId": "0183f9f0-3ab5-42c2-fc39-88c3678d443e"
   },
   "outputs": [],
   "source": [
    "# Plot the generated stories from the document-term matrix.\n",
    "pca = PCA(n_components=2)\n",
    "plot_data = pca.fit_transform(dtm)\n",
    "xs, ys = plot_data[:, 0], plot_data[:, 1]\n",
    "\n",
    "fig = plt.figure(figsize=(20, 15))\n",
    "plt.clf()\n",
    "plt.title(\"PCA of Generated Stories\")\n",
    "plt.style.use('ggplot')\n",
    "plt.scatter(xs, ys, marker = 'o')\n",
    "for i in range(dtm.shape[0]):\n",
    "         plt.annotate(i, xy = (xs[i], ys[i]), xytext = (3, 3),\n",
    "            textcoords = 'offset points', ha = 'left', va = 'top')\n",
    "dtm.shape[0]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "uZxWCQa7drD7",
    "outputId": "ba705135-b94b-4d8b-a31b-32ae3253d50b"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "for story in outputs:\n",
    "  display(HTML('<div>' + story + '</div>'))\n",
    "  display(HTML('<hr>'))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
