{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "73b4L-4goMpH"
   },
   "outputs": [],
   "source": [
    "!pip install -U dartmouth-langchain > /dev/null\n",
    "from dartmouth_langchain.llms import ChatDartmouth, DartmouthLLM\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(prompt):\n",
    "    \"\"\"\n",
    "    This function will format a prompt into what is needed for LangChain to produce ChatML.\n",
    "    Args:\n",
    "       prompt: the text to be embedded as human prompt.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        SystemMessage(content=system_prompt),\n",
    "        HumanMessage(content=prompt),\n",
    "    ]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "na63oEfcn1qR"
   },
   "outputs": [],
   "source": [
    "os.environ[\"DARTMOUTH_API_KEY\"] = \"KEY_GOES_HERE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q-22dEseo-No"
   },
   "outputs": [],
   "source": [
    "# Less stochasticity, more reliable output\n",
    "max_new_tokens = 1024\n",
    "top_p = 0.95\n",
    "temperature = .55\n",
    "system_prompt = \"Always assist with care, respect, and truth. Respond with utmost utility yet securely. Avoid harmful, unethical, prejudiced, or negative content. Ensure replies promote fairness and positivity.\"\n",
    "kwargs = dict()\n",
    "\n",
    "llm_chat = ChatDartmouth(model_name=\"llama-3-1-8b-instruct\",\n",
    "                    temperature = temperature,\n",
    "                    top_p = top_p,\n",
    "                    max_tokens = max_new_tokens,\n",
    "                    model_kwargs = kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Prompting for completion\n",
    "prompt = \"\"\"Instructions: please complete the following text. Use only what you have seen in your training data.\n",
    "\n",
    "The woods are lovely, dark and deep,   \n",
    "But I have promises to keep,\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llm_chat.invoke(format_prompt(prompt)).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Masked Prompting for completion\n",
    "prompt = \"\"\"Instructions: In the following replace [MASK] with the correct word. Use only a single word.\n",
    "Make sure to use only the words found from samples in your training data. You must make a guess, \n",
    "even if you are uncertain.\n",
    "\n",
    "Example: \n",
    "\n",
    "Input: Whenever Richard [MASK] went down town,\n",
    "Output <token>Cory</token>\n",
    "\n",
    "Input: But a caged [MASK] stands on the grave of dreams\n",
    "Output:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llm_chat.invoke(format_prompt(prompt)).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Accessing Logprobs for next token prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"What is the next token. Provide just a single word. \n",
    "Dartmouth College's mission is to educate promising students and prepare them for a \n",
    "lifetime of learning and responsible\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify params to include logprobs\n",
    "llm_chat = ChatDartmouth(model_name=\"llama-3-1-8b-instruct\",\n",
    "                    temperature = 1.5,\n",
    "                    top_p = top_p,\n",
    "                    logprobs = True,\n",
    "                    top_logprobs = 5,\n",
    "                    max_tokens = max_new_tokens,\n",
    "                    model_kwargs = kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign output to a variable\n",
    "output = llm_chat.invoke(format_prompt(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gain access to logprobs\n",
    "logprobs = output.response_metadata['logprobs']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a DataFrame of the logprobs, padding if necessary\n",
    "rows = list()\n",
    "for token in logprobs[:10]:\n",
    "    tks = []\n",
    "    for t in token['top_logprobs']:\n",
    "        tks.append(t['token'] + \" (\" + str(np.round(t['logprob'],3)) + \")\")\n",
    "    tp = len(token['top_logprobs'])\n",
    "    pad = 5 - tp\n",
    "    for i in range(pad):\n",
    "        tks.append(\"NA\")\n",
    "    rows.append(tks)\n",
    "df = pd.DataFrame(rows,columns=[\"Selected\",\"Tk1\",\"Tk2\",\"Tk3\",\"Tk4\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
