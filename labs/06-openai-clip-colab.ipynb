{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0750c6be-7b9d-413a-a532-da856818bfea",
   "metadata": {},
   "source": [
    "# <center>Critical AI</center>\n",
    "<center>ENGL 54.41</center>\n",
    "<center>Dartmouth College</center>\n",
    "<center>Fall 2024</center>\n",
    "<pre>Created: 06/19/2023; Revised: 10/02/2024</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06539a6-a6a5-4382-97ff-d58c54754749",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import CLIPProcessor, CLIPModel, CLIPTokenizer\n",
    "import torch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b93bed-5ea2-4775-8e6f-cf9b08c16dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell of code will determine if we have an accelerator for running\n",
    "# our neural networks.\n",
    "# mps == Apple Silicon device (MX series of Macbooks)\n",
    "# cuda == Compute Unified Device Architecture is a toolkit from Nvidia and means we have a GPU\n",
    "# cpu == Just using the general-purpose CPU for our calculations\n",
    "\n",
    "if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print('Using device: {0}'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8e6b0b-9921-4d76-a16f-df7d17b9f532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are loading a transformer neural network (more on this architecture later this term)\n",
    "# there are three components that we need: the model, the image processor, and the tokenizer\n",
    "# we'll learn more about tokenization later, for now just know that this \n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\",\n",
    "                                  torch_dtype=torch.float16,\n",
    "                                  device_map=\"auto\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\",\n",
    "                                          torch_dtype=torch.float16,\n",
    "                                          clean_up_tokenization_spaces=True,\n",
    "                                          device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398e0e80-b543-4e8b-bf25-3562cc2efc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(image,labels):\n",
    "    img = Image.open(image)\n",
    "    inputs = processor(text = labels,\n",
    "                       images = img, \n",
    "                       return_tensors = \"pt\",\n",
    "                       padding = True) \n",
    "    outputs = model(**inputs.to(device))\n",
    "    logits = outputs.logits_per_image\n",
    "    probs = logits.softmax(dim = 1)\n",
    "    # fix for float16 data\n",
    "    return [(labels[c],np.round(probs[0][c].detach().to('cpu').numpy(),3)) \n",
    "            for c in torch.argsort(probs,descending=True)[0]][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37cefbb-c866-4c70-af07-567aa4136a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a sample image\n",
    "!wget https://raw.githubusercontent.com/jeddobson/ENGL54.41-24F/6ebd8c683c3b0d230f16e99fdf7baa2113d10822/img/hood-03-library.jpg\n",
    "img = Image.open(\"hood-03-library.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3caefe-6d3a-49c5-9358-92fd8a3ca437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's display the image.\n",
    "# This is Lori Nix, American, born 1969 | Kathleen Gerber, American, born 1967\n",
    "# Library 2007, 2018.37.261, Hood Museum of Art. Dartmouth College.\n",
    "# https://hoodmuseum.dartmouth.edu/objects/2018.37.261\n",
    "plt.imshow(img)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0050de-3c0d-4aa1-b3c4-ee671eaa904e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we did with VGG16, we need to preprocess the image. For now,\n",
    "# we can supply just a sample stand-in caption.\n",
    "inputs = processor(text = [\"Library\"],\n",
    "                       images = img, \n",
    "                       return_tensors = \"pt\",\n",
    "                       padding = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6427c626-1ec7-4b57-9aa9-0ed4d290c573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what is returned...\n",
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2d412e-1f4d-434a-b8f2-a954430eb819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will learn more about these 'input_ids' when we get into\n",
    "# Transformers more in depth. For now, understand that we are\n",
    "# getting back representations of the text (descriptions), the\n",
    "# images (as pixel values) and something called an attention_mask\n",
    "# that can be used to filter pixel values for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4d5a05-9725-490e-9235-998f4624452e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's view the pre/processed image. Do you notice any similarities\n",
    "# and differences from what we saw in the VGG16 CNN preprocessed\n",
    "# data? What else do you see?\n",
    "plt.imshow(inputs['pixel_values'][0].permute(1,2,0))\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee91b2dc-0480-4fec-93f0-fd9801fa1144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay, now we are going to define some possible labels as descriptions.\n",
    "# \n",
    "captions = [\"This is a photograph of a library\",\n",
    "            \"This is a photograph of a library taken over by nature\",\n",
    "            \"This is a realistic drawing of a library\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaeb1ab-3164-4e0a-9dae-bb1916aacf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what do you see here? Anything of interest?\n",
    "classify(\"hood-03-library.jpg\",captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4ddf98-6af9-4b4d-b60c-a1b9662f91d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = [\"This is a photograph of a library\",\n",
    "            \"This is a photograph of a library taken over by nature\",\n",
    "            \"This is a realistic drawing of a library\",\n",
    "           \"This is a photograph of a diorama of a library\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc498c1-3045-4841-af3e-384861faabae",
   "metadata": {},
   "outputs": [],
   "source": [
    "classify(\"hood-03-library.jpg\",captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1382c588-7455-4fad-ae8a-eea032656d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = [\"This is a photograph of a library\",\n",
    "            \"This is a photograph of a library taken over by nature\",\n",
    "            \"This is a realistic drawing of a library\",\n",
    "            \"This is a photograph of a diorama of a library\",\n",
    "           \"This is a photograph of a diorama of a library taken over by nature\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b7eba3-06e9-4782-ae28-9d48904666ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "classify(\"hood-03-library.jpg\",captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cb0145-5022-4cea-b7fa-6f674b5a8cc4",
   "metadata": {},
   "source": [
    "## Try it! \n",
    "\n",
    "Now use the image that you previously uploaded or upload another image and try some experiments using the classify() function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
