{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Critical AI</center>\n",
    "<center>ENGL 54.41</center>\n",
    "<center>Dartmouth College</center>\n",
    "<center>Fall 2024</center>\n",
    "\n",
    "<hr>\n",
    "\n",
    "Notes: This notebook creates a modified Perceptron-style network with 10 outputs (rather than 2, as in the previous notebook for a binary classifier) to enable the classification of the full MNIST dataset. It also displays a more visually exciting confusion matrix resulting from the test dataset.\n",
    "\n",
    "<pre>Created: 07/15/2024; Revised: 09/25/2024</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import struct\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell of code will determine if we have an accelerator for running\n",
    "# our neural networks.\n",
    "# mps == Apple Silicon device (MX series of Macbooks)\n",
    "# cuda == Compute Unified Device Architecture is a toolkit from Nvidia and means we have a GPU\n",
    "# cpu == Just using the general-purpose CPU for our calculations\n",
    "\n",
    "if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print('Using device: {0}'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** Important Variables **\n",
    "\n",
    "# number of training iterations\n",
    "epochs = 100\n",
    "\n",
    "# learning rate \n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define the Perceptron as three layers: \n",
    "# input, hidden, output\n",
    "class Perceptron(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Perceptron, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 128)    # our first layer (S) will take as input pixel data and output 128 values\n",
    "        self.layer2 = nn.Linear(128, 128)          # this is our hidden layer (A) that takes 128 input values and outputs the same\n",
    "        self.layer3 = nn.Linear(128, 10)            # this is our final layer (R) that will take 128 input values and output 10 values\n",
    "\n",
    "    # define forward function with non-linear activiation. This is more complicated than the \n",
    "    # simple linear activation function that we used in the last notebook. The Rectified Linear Unit\n",
    "    # or ReLU function is applied to the weights (and later updated by the optimizer) as we push\n",
    "    # our data through the network.\n",
    "    def forward(self, inputs):\n",
    "        inputs = torch.relu(self.layer1(inputs))\n",
    "        inputs = torch.relu(self.layer2(inputs))\n",
    "        outputs = self.layer3(inputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST data from\n",
    "# http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "with open('../data/train-images-idx3-ubyte','rb') as f:\n",
    "    magic, size = struct.unpack(\">II\", f.read(8))\n",
    "    nrows, ncols = struct.unpack(\">II\", f.read(8))\n",
    "    data = np.fromfile(f, dtype=np.dtype(np.uint8).newbyteorder('>'))\n",
    "    data = data.reshape((size, nrows, ncols))\n",
    "\n",
    "with open('../data/train-labels-idx1-ubyte','rb') as f:\n",
    "    magic, size = struct.unpack(\">II\", f.read(8))\n",
    "    labels = np.fromfile(f, dtype=np.dtype(np.uint8).newbyteorder('>'))\n",
    "\n",
    "# display information about the dataset\n",
    "print(f'training samples: {data.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the testing data\n",
    "with open('../data/t10k-images-idx3-ubyte','rb') as f:\n",
    "    magic, size = struct.unpack(\">II\", f.read(8))\n",
    "    nrows, ncols = struct.unpack(\">II\", f.read(8))\n",
    "    test_data = np.fromfile(f, dtype=np.dtype(np.uint8).newbyteorder('>'))\n",
    "    test_data = test_data.reshape((size, nrows, ncols))\n",
    "\n",
    "with open('../data/t10k-labels-idx1-ubyte','rb') as f:\n",
    "    magic, size = struct.unpack(\">II\", f.read(8))\n",
    "    test_labels = np.fromfile(f, dtype=np.dtype(np.uint8).newbyteorder('>')) \n",
    "\n",
    "# display information about the dataset\n",
    "print(f'testing samples: {test_data.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display an image from training data from each class\n",
    "idxs = [labels.tolist().index(i) for i in range(10)]\n",
    "fig = plt.figure(figsize=(10, 5))  # width, height in inches\n",
    "for i,idx in enumerate(idxs):\n",
    "    img = fig.add_subplot(2, 5, i + 1)\n",
    "    img.imshow(data[idx].reshape(28,28).astype('uint8'), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape our training data\n",
    "y = labels.tolist()\n",
    "X = data.reshape(60000,784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating testing data \n",
    "y_test = test_labels.tolist()\n",
    "X_test = test_data.reshape(10000,784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model with input layer size dynamically set to length of data\n",
    "print(\"Creating neural network...\")\n",
    "input_size = X[0].shape[0]\n",
    "print(\"input layer size: {0}\".format(input_size))\n",
    "model = Perceptron(input_dim = input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data and labels to Torch tensor datatype\n",
    "training_data = torch.FloatTensor(X)\n",
    "labels = torch.LongTensor(y)\n",
    "\n",
    "test_labels = torch.LongTensor(y_test)\n",
    "testing_data = torch.FloatTensor(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates loss entropy for classification tasks\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# the Adam optimizer adjusts weights using gradient optimization  \n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "model.train()\n",
    "\n",
    "# iterate through each of the training epochs\n",
    "for e in range(epochs):\n",
    "    model.zero_grad()\n",
    "    outputs = model(training_data.to(device))\n",
    "\n",
    "    # supply labels to CrossEntropyLoss\n",
    "    loss_train = loss_fn(outputs.to('cpu'), labels)\n",
    "    loss_train.backward()\n",
    "\n",
    "    # adjust weights\n",
    "    optimizer.step()\n",
    "\n",
    "    if e % 10 == 0:\n",
    "        print(\"Epoch: {0} Loss: {1:.4f}\".format(e,loss_train.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to predict class from output\n",
    "def predict(input_data):\n",
    "    outputs = model(input_data.to(device))\n",
    "    pred = torch.argmax(outputs)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict entire testing dataset\n",
    "scores = list()\n",
    "for i, j in enumerate(test_labels):\n",
    "    pc = predict(testing_data[i])\n",
    "    scores.append([pc,test_labels[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fancy report from Scikit-Learn\n",
    "#  \n",
    "# precision = measures actual positive predictions (true positive / true postive + false positive)\n",
    "# recall = proportion of true positive predictions (true positive / true positive + false negative)\n",
    "# f1 = mean of precision and recall\n",
    "# support = number of samples in this class\n",
    "#\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "pred = [int(s[0]) for s in scores]\n",
    "print(\"Final Accuracy: {0}\".format(accuracy_score(test_labels, pred)))\n",
    "print(classification_report(test_labels, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Confusion Matrix\n",
    "\n",
    "The confusion matrix tells us much more about our model and data than the accuracy report. It enables us to understand which classes of objects might be similar enough to each other to \"confuse\" the model. It provides a good indicator of performance of the classifier by reading the values along the diagonal (correct or true positive classifications). You can understand much about the relationship between your features and classes by reading and interpreting the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the matrix as a simple table\n",
    "print(confusion_matrix(test_labels, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It looks much nicer as a visual with a heatmap and annotations.\n",
    "import seaborn as sn\n",
    "cm = confusion_matrix(test_labels, pred)\n",
    "sn.heatmap(cm,annot=True,cmap='Reds',fmt='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is how we can check a single vector\n",
    "sample = 1032\n",
    "print(f'Label: {test_labels[sample]}')\n",
    "print(f'Prediction: {predict(testing_data[sample])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can get probabilities of the classes with argsort:\n",
    "torch.argsort(model(testing_data[sample].to(device)),descending=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
